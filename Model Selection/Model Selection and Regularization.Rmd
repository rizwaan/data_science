---
title: "Model Selection and Regularization"
author: "Rizwaan Adil"
date: "February 16, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Model Selection Techiques

Demo of below techniques  

- Best Subset Selection  
- Forward Stepwise Selection  
- Model Selection Using a Validation Set  
- Cross Validation  
- Ridge Regression and the Lasso  

##### Pacakge Management and Data

```{r, message=FALSE}

library(ISLR)

summary(Hitters)
```


REmove the NAs in Salary


```{r}

Hitters=na.omit(Hitters)
sum(is.na(Hitters$Salary))  # Check the NAs
attach(Hitters)

```


##### Best Subset Selection 


The leaps package provides best subset modeling functions

```{r}

library(leaps)
regfit.full<-regsubsets(Salary~.,data = Hitters)
summary(regfit.full)


```

By default it takes max 8 variables for subsetting.  
Lets increase it to the full 19 variables

```{r}

regfit.full<-regsubsets(Salary~.,data = Hitters,nvmax = 19)
reg.summary<-summary(regfit.full)
names(reg.summary)


```



Plot the n Cp vs Variables  
```{r}

plot(reg.summary$cp,xlab = "Number of Variables", ylab = "Cp")
which.min(reg.summary$cp) # Identify lowest Cp. It is the tenth point
points(10,reg.summary$cp[10],pch=20,col="red")

```



Another plot for the model fit  

```{r}

plot(regfit.full,scale = "Cp")

```

This plot also confirms that the best model with best Cp has 10 variables.  

Now lets extract the coeff of this model  
 
```{r}

coefficients(regfit.full,10)


```


These coefficients thus extracted can be used to generate the production model

***

##### Forwards Stepwise Selection

Use the same regsubset function with forward opton

```{r}

regfit.fwd<-regsubsets(Salary~., data = Hitters,nvmax = 19,method = "forward")
summary(regfit.fwd)

```


Plot the model  


```{r}

plot(regfit.fwd,scale = "Cp")
regfit.fwd.summary<-summary(regfit.fwd)
```





```{r}
plot(regfit.fwd.summary$cp,xlab = "Number of Variables", ylab = "Cp")
which.min(regfit.fwd.summary$cp)
points(10,regfit.fwd.summary$cp[10],pch=20,col="red")
```



Extract the coeff

```{r}
coefficients(regfit.fwd,10)
```

***
#####Model Selection using Validation Set



Get the training samples

```{r}
set.seed(1)
train<-sample(seq(263),180,replace = FALSE)

```


Use forward stepwise selection on the training set.  
As usual this would generate 19 models with their own errors  


```{r}
regfit.fwd<-regsubsets(Salary~.,data=Hitters[train,],nvmax = 19,method = "for")


```


```{r}
val.errors = rep(NA, 19)
x.test = model.matrix(Salary ~ ., data = Hitters[-train, ])  # Generate a model matrix for test data
x.test

```


Calculate errors for all the models.

```{r}
for (i in 1:19) {
  coefi<-coef(regfit.fwd,i)  # get coeff of the ith model for prediction
  pred<-x.test[,names(coefi)] %*% coefi # Predict Salary for the test set
  val.errors[i] = mean((Hitters$Salary[-train] - pred)^2) # Calculate MSR for each model
}

val.errors

```


Plot the training and validation errors

```{r}

plot(sqrt(val.errors), ylab = "Root MSE", ylim = c(300, 400), pch = 19, type = "b")

points(sqrt(regfit.fwd$rss[-1]/180), col = "blue", pch = 19, type = "b")
legend("topright", legend = c("Training", "Validation"), col = c("blue", "black"), 
    pch = 19)



```







***
##### Cross Validation

Demo : 10 Folds cross validation

```{r}

set.seed(11)
folds<-sample(rep(1:10,length=nrow(Hitters)))
folds

```


```{r}
table(folds)
```



```{r}
cv.errors<-matrix(NA,10,19)
cv.errors

```

```{r}

#define a predict function for regsubset model
predict.regsubsets = function(object, newdata, id, ...) {
    form = as.formula(object$call[[2]])
    mat = model.matrix(form, newdata)
    coefi = coef(object, id = id)
    mat[, names(coefi)] %*% coefi
}


for (k in 1:10) {
    best.fit = regsubsets(Salary ~ ., data = Hitters[folds != k, ], nvmax = 19, 
        method = "forward")
    for (i in 1:19) {
        pred = predict.regsubsets(best.fit, Hitters[folds == k, ], id = i)
        cv.errors[k, i] = mean((Hitters$Salary[folds == k] - pred)^2)
    }
}

rmse.cv <- sqrt(apply(cv.errors, 2, mean))
plot(rmse.cv, pch = 19, type = "b")


```




***
##### Ridge Regression and Lasso


Package glmnet doesnt use model formula language. Hence matrices need to be set up separately

```{r, message=FALSE}

library(glmnet)
```

Create input matrix

```{r}

x = model.matrix(Salary ~ . - 1, data = Hitters)
y = Hitters$Salary




```




Fit ridge regression model (L2 Norm Regularization)  
Alpha=0 is ridge regression  
Alpha=1 is Lasso  
```{r}

fit.ridge<-glmnet(x,y,alpha=0)
plot(fit.ridge,xvar = "lambda",label = TRUE)




```


This plot confirms the argument that as lambda increases it suppresses the coefficients towards zero.  


Lets use cross validation to determine the best value for lambda  
```{r}

cv.ridge<-cv.glmnet(x,y,alpha=0) # 10 Fold CV by default
plot(cv.ridge)

```

The left most horizontol line corresponds to the lowest MSE lambda. We generally dont use that.  
The other horizontal line identifies lambda \ MSE withn 1 standard dev of the min MSE.  



****
Time to fit a lasso model

```{r}

fit.lasso<-glmnet(x,y,alpha = 1)
plot(fit.lasso,xvar = "lambda",label = TRUE)

```

As lambda increases, lasso makes many coeff 0 thereby removing variables from the model.  
Note the number of variables decrease on the alternate x axis  

Instead of lambda, we can plot Coeff against Adj R squared

```{r}
plot(fit.lasso,xvar = "dev",label = TRUE)

```



As usual, it is best to use cv to determine best lambda


```{r}

cv.lasso<-cv.glmnet(x,y,alpha=1)
plot(cv.lasso)

```

Lasso identifies the best lambda at around 4 seleting only 6 variables.

Coefficients for this best model can be extracted as below

```{r}
coef(cv.lasso)

```





***



Prediction using lasso



```{r}

lasso.tr<-glmnet(x[train,],y[train]) #CV is inbuilt
lasso.tr

```

Lasso select a model with 20 vars and lambda=0.3103


Prediction

```{r}
pred<-predict(lasso.tr,x[-train,])
dim(pred)


```

This has returned a matrix of coefficients   

Lets calculate and plot RMSE as a function of lambda

```{r}

rmse<-sqrt(apply((y[-train]-pred)^2,2,mean))
plot(log(lasso.tr$lambda),rmse,type="b")

```

As expected RMSE drops at first and then goes up as we increase lambda  


Extract the best lambda value  
```{r}
lambda.best<-lasso.tr$lambda[order(rmse)[1]]
lambda.best

```

Lets extract the coeff for the model with this best lambda

```{r}

coef(lasso.tr,s=lambda.best)

```




#### TODO  
- Principal Components regression
- Partial least square regression