---
title: "Text Mining with R - Chapter 5"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
[Source : Text Mining With R](http://tidytextmining.com/tidytext.html#the-unnest_tokens-function)



### Converting to and from Nontidy Formats

Tidytext is a relatively new package which has introduced the tody format for text corpus.
Conversion from tidy to non-tidy format is required to ensure compatibility with other packages.

e.g the TM package relies on a Document Term Matrix format , after which all NLP analysis is possible.

<br>
<br> 

#### Tidying a document-term matrix

Tidytext package provides below functions for this purpose  

- tidy() turns a document-term matrix into a tidy data frame  
- cast() turns a tidy corpus into dtm  
- cast_sparse() turns a tidy corpus into a sparse dtm  


A typical workflow for the overall analysis is as below  
<br>
<br>
![Workflow](format.png)



##### Tidying DocumentTermMatrix Objects


Lets work with the tm package data to showcase the conversion

```{r, message=FALSE}

library(tm)
library(topicmodels)

data("AssociatedPress")

AssociatedPress


```

This is a very sparse matrix as seen here.

To use tidy tools for analysis this text, we need to convert it into tidy format


```{r, message=FALSE}

library(dplyr)
library(tidytext)

ap_td <- tidy(AssociatedPress)
ap_td


```

The tidy version omits all zero entries in a document compared to a DTM.

Lets analyze the sentiments

```{r}


ap_sentiment <- ap_td %>% 
  inner_join(get_sentiments("bing"),by=c(term="word"))

ap_sentiment
```

Lets plot the common words in either direction


```{r, message=FALSE}

library(ggplot2)
library(ggthemes)

ap_sentiment %>% 
  count(sentiment,term,wt=count,sort=TRUE) %>% 
  filter(n>200) %>% 
  mutate(n=if_else(sentiment=="negative",-n,n)) %>% 
  mutate(term=reorder(term,n)) %>% 
  ggplot(aes(term,n,fill=sentiment))+
  geom_bar(stat="identity")+
  coord_flip()
  

  



```

##### Tidying dfm objects


An alternative to DTMs of the TM package is the dfm (document-feature matrix) class of quanteda package

```{r, message=FALSE}

library(quanteda)

data("inaugTexts",package = "quanteda")

inaug_dfm <- dfm(inaugTexts,verbose = FALSE)
inaug_dfm



```





Lets tidy up this corpus

```{r}

tidy(inaug_dfm)

inaug_td <- tidy(inaug_dfm) %>% 
  anti_join(stop_words,by=c(term="word"))

inaug_td

```




```{r}

inaug_tf_idf <- inaug_td %>% 
  bind_tf_idf(term,document,count) %>% 
  arrange(desc(tf_idf))

inaug_tf_idf

```



Lets plot imp words from a handful of addresses


```{r}

library(ggplot2)


inaug_tf_idf_plot <- inaug_tf_idf %>% 
  filter(document %in% c("1933-Roosevelt", "1861-Lincoln",
              "1961-Kennedy", "2009-Obama")) %>% 
  
  group_by(document) %>% 
  top_n(10,tf_idf) %>% 
  ungroup() %>%
  mutate(term=reorder(term,tf_idf)) 

  
inaug_tf_idf_plot %>% 
  ggplot(aes(term,tf_idf,fill=document)) +
  geom_col()+
  facet_wrap(~document,scales="free")+
  coord_flip()
  
  

```

Note: Quanteda's tokenizer includes ? as a term.


Another example where we visualize the terms by year


```{r}

library(tidyr)
year_term_counts <- inaug_td %>% 
  extract(document,"year","(\\d+)",convert=TRUE) %>% 
  complete(year,term,fill=list(count=0)) %>% 
  group_by(year) %>% 
  mutate(year_total=sum(count))
  


year_term_counts


```

Lets pick a few words and visualize their frequency over the years


```{r}

year_term_counts %>%
  filter(term %in% c("god", "america", "foreign", "union", "constitution", "freedom")) %>%
  
  ggplot(aes(year,count/year_total))+
  geom_line()+
  geom_smooth()+
  facet_wrap(~term,scales = "free")+
  scale_y_continuous(labels = scales::percent_format())+
  ylab("Percent freq of word in inaugaral address ")


```



### Casting tidy text data into a matrix

Some pakages might require DTMs as input.
Tidytext provides some function to facilitate this.


```{r}

ap_td %>% 
  cast_tdm(term,document,count)



```

Like TDM, there is an alternate DTM and DFM function too.


e.g we can generate a sparse matrix

```{r}

ap_td %>% 
  cast_sparse(document,term,count) %>% 
  dim()

```

Note the relatively fewer rows in this sparse matrix compared to 302k entries previously

Another example..

```{r}

library(janeaustenr)

austen_dtm <- austen_books() %>% 
  unnest_tokens(word,text) %>% 
  count(book,word) %>% 
  cast_dtm(book,word,n)


austen_dtm


```



### Tidying corpus objects with metadata


Apart from DTM objects, the tm package supports corpus objects.  
Corpus contains metadata along with text.  
While converting to tidy format, we can extract metadata from corpus objects as well.  


Lets play with the acq dataset from tm package which is a collection of 50 news articles from Reuters

```{r, message=FALSE}

library(tm)

data(acq)
acq

```

Sample a single document

```{r}
acq[[1]]

```


Actual content of this document

```{r}
inspect(acq[[1]])


```

Metadata for this document

```{r}
meta(acq[[1]])

```



Converting to tidy format gives all metadata and text in one shot


```{r}

acq_td <- tidy(acq)

acq_td

```

We can then unnest tokens as always


```{r}

acq_tokens <- acq_td %>% 
  select(-places) %>%  ## unnest throws error if any column is a list
  unnest_tokens(word,text) %>% 
  anti_join(stop_words,by="word")

acq_tokens

```

Lets find some common words

```{r}

acq_tokens %>% 
  count(word,sort=TRUE)

```

Generate tfidf

```{r}
acq_tokens %>% 
  count(id,word,sort=TRUE) %>% 
  bind_tf_idf(word,id,n)
  
  

```



#### Example: Mining Financial Articles


We will web mine finance articles for some major tech companies in this section.

```{r, message=FALSE}


library(tm.plugin.webmining)
library(purrr)

company <- c("Microsoft", "Apple", "Google", "Amazon", "Facebook")#,"Twitter") #, "IBM", "Yahoo", "Netflix")
symbol <- c("MSFT", "AAPL", "GOOG", "AMZN", "FB")#  , "TWTR")#,  "IBM", "YHOO", "NFLX")

download_articles <- function(symbol) {
  WebCorpus(GoogleFinanceSource(paste0("NASDAQ:", symbol)))
}




stock_articles <- data_frame(company,symbol) %>% 
  mutate(corpus=map(symbol,download_articles))


stock_articles




```

This dataframe holds the webcorpus in the last column.

P.S: More on the [map functions](http://r4ds.had.co.nz/iteration.html#the-map-functions)


Lets tokeninze these corpus and append to a single dataframe

```{r}


# map produces a list of 5 tibbles each of 20X8 dimension. Unnest unlists the elements and repeats every row of these tibbles for each company-symbol combination. It also produces a single dataframe

stock_tokens <- stock_articles %>% 
  unnest(map(corpus,tidy)) %>% 
  unnest_tokens(word,text) %>% 
  select(company, datetimestamp, word, id, heading)

stock_tokens




```

Lets find the words most specific to these stocks using tfidf

```{r,message=FALSE}

library(stringr)


stock_tfidf <- stock_tokens %>% 
  count(company,word) %>% 
  filter(!str_detect(word,"\\d")) %>%  # lot of numbers seen as tokens
  bind_tf_idf(word,company,n) %>% 
  arrange(-tf_idf)


stock_tfidf


```



Now that we have the tfidfs lets plot by company


```{r}
stock_plot <- stock_tfidf %>% 
  group_by(company) %>% 
  top_n(10,tf_idf) %>%  ## Explicitly specifying the sort order
  ungroup() %>% 
  mutate(word=reorder(word,tf_idf)) %>% 
  ggplot(aes(word,tf_idf,fill=company))+
  geom_col(show.legend = FALSE)+
  coord_flip()+
  facet_wrap(~company,scales = "free_y")

stock_plot


```

These plots show the most characteristic words for the companies on any given day.



We can further quantify the overall sentiment of the press coverage for these companies.
This can be done by taking the ratio of difference between positive and negative words over the total number of words.

Lets try this using the AFINN lexicon and find which words contribute the most to  either sentiment.


```{r, message=FALSE}


stock_tokens %>% 
  anti_join(stop_words,by="word") %>% 
  count(word,id,sort=TRUE)%>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(word) %>% 
  summarise(contribution=sum(n*score)) %>% 
  top_n(15,abs(contribution)) %>% 
  mutate(word=reorder(word,contribution)) %>% 
  ggplot(aes(word,contribution))+
  geom_col()+
  coord_flip()+
  ylab("Word Freq * Sentiment Score")

  



```

This chart shows that AFINN is not a suitable lexicon for finance text.  
e.g share is a positive word wherein it should have been a neutral term. Similarly fool (refers to Motley Fool publication) is incorrectly categorized as a negative term.  

To overcome this we must use the  Loughran and McDonald dictionary of financial sentiment terms.  
This dictionary was developed based on analyses of financial reports, and intentionally avoids words like “share” and “fool,” as well as subtler terms like “liability” and “risk” that may not have a negative meaning in a financial context.  


The Loughran data divides words into six sentiments: “positive,” “negative,” “litigious,” “uncertain,” “constraining,” and “superfluous.”

```{r}
library(tidytext)

stock_tokens %>% 
  count(word) %>% 
  inner_join(get_sentiments("loughran"),by="word") %>% 
  group_by(sentiment) %>% 
  top_n(5,n) %>% 
  ungroup() %>% 
  mutate(word=reorder(word,n)) %>% 
  ggplot(aes(word,n,fill=sentiment))+
  geom_col(show.legend = FALSE)+
  coord_flip()+
  facet_wrap(~sentiment,scales="free")
  
  


```


These financial sentiments can further be explored to guage the overall press sentiment


```{r, message=FALSE  }

stock_sentiment_count <- stock_tokens %>% 
  inner_join(get_sentiments("loughran"), by = "word") %>%
  count(sentiment,company) %>% 
  spread(sentiment,n,fill=0)

stock_sentiment_count

```


```{r}

stock_sentiment_count %>% 
  mutate(score=(positive-negative)/(positive+negative)) %>% 
  mutate(company=reorder(company,score)) %>% 
  ggplot(aes(company,score,fill=score>0)) +
  geom_col(show.legend = FALSE)+
  coord_flip()
  
  

```

Thus we can conclude that in early July 2017, Google had a comparitively negative press sentiment as compared to MS and Apple


